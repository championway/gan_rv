{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from shutil import copyfile\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import data\n",
    "from skimage import exposure\n",
    "from skimage.transform import match_histograms\n",
    "from models import *\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "G_AB = GeneratorResNet(res_blocks=9)\n",
    "if cuda:\n",
    "    G_AB = G_AB.cuda()\n",
    "G_AB.load_state_dict(torch.load('/media/arg_ws3/5E703E3A703E18EB/research/cycle_box_sim2real/saved_models/box_sim2real_G_LAMDAID_1/G_AB_1031525_.pth'))\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
    "\n",
    "data_transform = transforms.Compose(\n",
    "                [ transforms.Resize((240, 320), Image.BICUBIC),\n",
    "                #transforms.RandomCrop((opt.img_height, opt.img_width)),\n",
    "                #transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grid(tensor, nrow=8, padding=2,\n",
    "              normalize=False, range=None, scale_each=False, pad_value=0):\n",
    "    \"\"\"Make a grid of images.\n",
    "\n",
    "    Args:\n",
    "        tensor (Tensor or list): 4D mini-batch Tensor of shape (B x C x H x W)\n",
    "            or a list of images all of the same size.\n",
    "        nrow (int, optional): Number of images displayed in each row of the grid.\n",
    "            The Final grid size is (B / nrow, nrow). Default is 8.\n",
    "        padding (int, optional): amount of padding. Default is 2.\n",
    "        normalize (bool, optional): If True, shift the image to the range (0, 1),\n",
    "            by subtracting the minimum and dividing by the maximum pixel value.\n",
    "        range (tuple, optional): tuple (min, max) where min and max are numbers,\n",
    "            then these numbers are used to normalize the image. By default, min and max\n",
    "            are computed from the tensor.\n",
    "        scale_each (bool, optional): If True, scale each image in the batch of\n",
    "            images separately rather than the (min, max) over all images.\n",
    "        pad_value (float, optional): Value for the padded pixels.\n",
    "\n",
    "    Example:\n",
    "        See this notebook `here <https://gist.github.com/anonymous/bf16430f7750c023141c562f3e9f2a91>`_\n",
    "\n",
    "    \"\"\"\n",
    "    if not (torch.is_tensor(tensor) or\n",
    "            (isinstance(tensor, list) and all(torch.is_tensor(t) for t in tensor))):\n",
    "        raise TypeError('tensor or list of tensors expected, got {}'.format(type(tensor)))\n",
    "\n",
    "    # if list of tensors, convert to a 4D mini-batch Tensor\n",
    "    if isinstance(tensor, list):\n",
    "        tensor = torch.stack(tensor, dim=0)\n",
    "\n",
    "    if tensor.dim() == 2:  # single image H x W\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "    if tensor.dim() == 3:  # single image\n",
    "        if tensor.size(0) == 1:  # if single-channel, convert to 3-channel\n",
    "            tensor = torch.cat((tensor, tensor, tensor), 0)\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "\n",
    "    if tensor.dim() == 4 and tensor.size(1) == 1:  # single-channel images\n",
    "        tensor = torch.cat((tensor, tensor, tensor), 1)\n",
    "\n",
    "    if normalize is True:\n",
    "        tensor = tensor.clone()  # avoid modifying tensor in-place\n",
    "        if range is not None:\n",
    "            assert isinstance(range, tuple), \\\n",
    "                \"range has to be a tuple (min, max) if specified. min and max are numbers\"\n",
    "\n",
    "        def norm_ip(img, min, max):\n",
    "            img.clamp_(min=min, max=max)\n",
    "            img.add_(-min).div_(max - min + 1e-5)\n",
    "\n",
    "        def norm_range(t, range):\n",
    "            if range is not None:\n",
    "                norm_ip(t, range[0], range[1])\n",
    "            else:\n",
    "                norm_ip(t, float(t.min()), float(t.max()))\n",
    "\n",
    "        if scale_each is True:\n",
    "            for t in tensor:  # loop over mini-batch dimension\n",
    "                norm_range(t, range)\n",
    "        else:\n",
    "            norm_range(tensor, range)\n",
    "\n",
    "    if tensor.size(0) == 1:\n",
    "        return tensor.squeeze()\n",
    "\n",
    "    # make the mini-batch of images into a grid\n",
    "    nmaps = tensor.size(0)\n",
    "    xmaps = min(nrow, nmaps)\n",
    "    ymaps = int(math.ceil(float(nmaps) / xmaps))\n",
    "    height, width = int(tensor.size(2) + padding), int(tensor.size(3) + padding)\n",
    "    grid = tensor.new_full((3, height * ymaps + padding, width * xmaps + padding), pad_value)\n",
    "    k = 0\n",
    "    for y in irange(ymaps):\n",
    "        for x in irange(xmaps):\n",
    "            if k >= nmaps:\n",
    "                break\n",
    "            grid.narrow(1, y * height + padding, height - padding)\\\n",
    "                .narrow(2, x * width + padding, width - padding)\\\n",
    "                .copy_(tensor[k])\n",
    "            k = k + 1\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(path):\n",
    "    \"\"\"Saves a generated sample from the test set\"\"\"\n",
    "    #imgs = next(iter(val_dataloader))\n",
    "    #image = cv2.imread(\"/media/arg_ws3/5E703E3A703E18EB/data/mm_unity/unity_boxes/box_108/Scene65/11_original.png\")\n",
    "    #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = Image.open(path)\n",
    "    #pil_im = Image.fromarray(image)\n",
    "    pil_im = data_transform(image)\n",
    "    pil_im = pil_im.unsqueeze(0)\n",
    "\n",
    "    #image = cv2.resize(image, (256, 256)) \n",
    "    #image = np.transpose(image, (2, 0, 1))\n",
    "    #image = torch.tensor(image)\n",
    "    #image = image.unsqueeze(0)\n",
    "\n",
    "    #print(image.shape)\n",
    "    #print(pil_im.shape)\n",
    "\n",
    "    my_img = Variable(pil_im.type(Tensor))\n",
    "    my_img_fake = G_AB(my_img)\n",
    "    my_img_fake = my_img_fake.squeeze(0).detach().cpu()\n",
    "    \n",
    "    grid = make_grid(my_img_fake, nrow=5, normalize=True)\n",
    "    pil_ = grid.mul_(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to('cpu', torch.uint8).numpy()\n",
    "    #print(pil_)\n",
    "    pil_ = np.array(pil_)\n",
    "    pil_ = pil_[...,::-1]\n",
    "    pil_ = cv2.resize(pil_, (640, 480))\n",
    "    return pil_\n",
    "    #print(pil_.dtype)\n",
    "    #cv2.imwrite(\"crop_mask1.jpg\", pil_)\n",
    "\n",
    "    #save_image(my_img_fake, 'boxxx.png', nrow=5, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/media/arg_ws3/5E703E3A703E18EB/data/mm_FCN/box_gan_LAMDAID_1/image/\"\n",
    "image_root = \"/media/arg_ws3/5E703E3A703E18EB/data/mm_FCN/unity/image/\"\n",
    "mask_root = \"/media/arg_ws3/5E703E3A703E18EB/data/mm_FCN/unity/mask/\"\n",
    "box_root = \"/media/arg_ws3/5E703E3A703E18EB/data/mm_FCN/unity/box/\"\n",
    "box_list = os.listdir(box_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for box in box_list:\n",
    "    mask_name = box.split('_original.png')[0] + '_seg.png'\n",
    "    #box_img = cv2.imread(os.path.join(box_root, box))\n",
    "    matched = sample_images(os.path.join(box_root, box))\n",
    "    image = cv2.imread(os.path.join(image_root, box))\n",
    "    mask_img = cv2.imread(os.path.join(mask_root, mask_name), 0)\n",
    "    mask_img[mask_img == 0] = 255\n",
    "    mask = cv2.cvtColor(mask_img, cv2.COLOR_GRAY2RGB)\n",
    "    matched = cv2.bitwise_and(matched, mask, mask=mask_img)\n",
    "    mask_img[mask_img < 255] = 1\n",
    "    mask_img[mask_img == 255] = 0\n",
    "    mask_img[mask_img == 1] = 255\n",
    "    mask = cv2.cvtColor(mask_img, cv2.COLOR_GRAY2RGB)\n",
    "    image = cv2.bitwise_and(image, mask, mask=mask_img)\n",
    "    result = cv2.add(matched, image)\n",
    "    cv2.imwrite(os.path.join(root, box), result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
